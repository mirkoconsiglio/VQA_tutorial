{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Variational Quantum Algorithms\n",
    "\n",
    "Quantum computing is a relatively young field, which bloomed as an idea to Benioff when he proposed the quantum mechanical analogue of a Turing machine, which was quickly followed up by Deutsch, and Feynman, among many others. Research in quantum computing flourished when Shor devised a quantum algorithm that efficiently determines prime factors of composite integers, which could result in the collapse of many modern cryptographic protocols. Since then, many quantum algorithms have been designed with varying applications, ranging from optimisation, machine learning, quantum many-body physics and chemistry, cryptography, communication and pharmaceuticals.\n",
    "\n",
    "## Computational Complexity Theory in a Nutshell\n",
    "\n",
    "Computational complexity theory is a means of assigning computational problems to different complexity classes, each with its own set of characteristics and limitations in terms of some resource, most commonly time and memory. For the sake of conciseness, the asymptotic notation of function complexity will be briefly described, and only the relevant complexity classes will be mentioned in this thesis. The most important three definitions for asymptotic complexity (as a function the size of the system $n$) are:\n",
    "\n",
    "- $f(n) = \\mathcal{O}(T(n))$, where $f$ is said to be asymptotically bounded above by $T$ (up to a multiplicative constant).\n",
    "- $f(n) = \\Omega(T(n))$, where $f$ is said to be asymptotically bounded below by $T$ (up to a multiplicative constant).\n",
    "- $f(n) = \\Theta(T(n))$, where $f$ is said to be asymptotically bounded above and below by $T$ (up to two multiplicative constants).\n",
    "\n",
    "Now let us turn our attention to complexity classes which are relevant to the algorithms discussed and mentioned in this thesis:\n",
    "- P: problems that can be solved in a polynomial time by a deterministic classical computer. This means that the problem can be solved in $\\mathcal{O}\\left(n^k\\right)~\\exists~k > 0$. Note that $k = 1$ denotes linear time whilst $0 < k < 1$ denotes fractional time.\n",
    "- NP: problems that can be verified in a polynomial time by a deterministic classical computer, but not necessarily solved in such a time. This implies that the problem can be verified in $\\mathcal{O}\\left(n^k\\right)~\\exists~k > 1$, however the solution cannot be determined faster than $\\Omega\\left(n^k\\right)~\\forall~k > 1$.\n",
    "- BQP: stands for Bounded-error Probabilistic Polynomial-time. Essentially equivalent to P but for a quantum computer.\n",
    "- PSPACE: stands for Polynomial Space. Problems that can be solved in a polynomial space by a deterministic classical computer.\n",
    "- EXPTIME: stands for Exponential Time. Problems that can be solved in an exponential time by a deterministic classical computer. This means that the problem can be solved in $\\mathcal{O}\\left(k^n\\right)~\\exists~k > 1$.\n",
    "- QMA: stands for Quantum Merlin--Arthur and is the quantum analogy for $NP$. A problem is said to be in $QMA$ if the solution can be verified in a polynomial time by a quantum computer, if it is given \"yes\" as an answer.\n",
    "\n",
    "The classes mentioned above can be neatly placed in containers as shown in the figure below. Furthermore, the last few important definitions are the concepts of reducibility, hardness and completeness. A problem $A$ is said to be reducible to a problem $B$, if a method for solving $B$ implies a method for solving $A$ in the same time. This is denoted as $A \\leq B$. This can also be interpreted to mean that solving $B$ is at least as hard as solving $A$. Given a complexity class $C$, a problem $X$ is said to be $C$-hard if every problem in $C$ reduces to $X$, and finally, a problem $X$ is said to be $C$-complete if $X$ is $C$-hard and also $X \\in C$.\n",
    "\n",
    "<img height=\"500\" src=\"figures/complexity_classes.png\" width=\"400\"/>\n",
    "\n",
    "A typical example of a BQP problem is integer factorisation, which is polynomially solvable by a quantum computer using Shor's algorithm. On the other hand, there is no known classical algorithm that can factor integers in polynomial time. However, it is crucial to carry out tests for quantum algorithmic complexity, as it is obscure whether quantum computers are able to solve NP-Complete problems in polynomial time. In fact, most of the algorithms which will be discussed in this work are related to the preparation of a desired initial state. It is established that preparing --- and also *finding* specific states, such as ground-states of Hamiltonians --- is QMA-hard.\n",
    "\n",
    "## The Era of Noise. Is there a Solution?\n",
    "\n",
    "While the fault-tolerant quantum computer era might herald in a new age of unparalleled efficiency through quantum algorithms, we are still a far cry away from achieving _true_ quantum advantage. In essence, quantum advantage refers to the ability of quantum computers to solve certain problems significantly faster than classical computers. Formally, it consists of two fundamental tasks: the engineering feat of constructing a robust quantum computer, and the computational complexity challenge of identifying a problem that can be efficiently solved by such a quantum computer, showcasing a superpolynomial speedup compared to the most efficient classical algorithms available for that specific task.\n",
    "\n",
    "Initially, most of the proposed quantum algorithms required millions of physical qubits to operate successfully and efficiently, along with the implementation of quantum error correction (QEC) protocols, to reduce the inherent decoherence of qubits, improper gate control and measurement errors --- collectively known as noise. Currently, we are in the era of noisy intermediate-scale quantum (NISQ) devices, where we only have access to around the order of 100 noisy qubits at most. As a result, most of the current quantum algorithms have to be devised with the current severe limitations in mind. In fact, the main goal in the NISQ era is to develop techniques and algorithms which are able to extract the maximum available quantum computational power, while also being suitable to scale for the future long-term goal of fault-tolerant quantum computation.\n",
    "\n",
    "Nevertheless, attempts at attaining quantum advantage, or quantum supremacy, have been made in the NISQ era. In particular, the main proposals for demonstrating quantum advantage typically fall under one of the following algorithms: factoring integers, notably using Shor's algorithm; boson sampling, a computing paradigm centred on the transmission of identical photons through a linear-optical network can effectively tackle specific sampling and search problems; and cross-entropy benchmarking, which consists of sampling the output distribution of random quantum circuits.\n",
    "\n",
    "For instance, Google claimed to have achieved quantum supremacy in 2019 with its 53-qubit quantum computer, Sycamore, demonstrating that it could solve a specific problem faster than the most powerful classical supercomputers at the time. In response, IBM indicated that certain claims were overstated and proposed a potential time frame of 2.5 days rather than 10,000 years for solving the same problem, detailing various techniques that a classical supercomputer could employ to enhance computational efficiency. IBM's input holds significance, especially considering that the most potent supercomputer at that time, Summit, was developed by IBM. Subsequent to this, researchers have refined algorithms for the sampling problem pivotal in asserting quantum supremacy, leading to significant reductions in the disparity between Google's Sycamore processor and classical supercomputers, even surpassing it in certain instances.\n",
    "\n",
    "In December 2020, researchers from the University of Science and Technology of China, led by Jian-Wei Pan, similarly, claimed to have achieved quantum supremacy by employing Gaussian boson sampling on 76 photons with their photonic quantum computer named Jiuzhang. According to the paper, the quantum computer generated a number of samples in 200 seconds that would necessitate a classical supercomputer 2.5 billion years to compute. In October 2021, researchers from the University of Science and Technology of China once more demonstrated quantum advantage by constructing two supercomputers named Jiuzhang 2.0 and Zuchongzhi. The Jiuzhang 2.0, which operates on light-based principles, employed Gaussian boson sampling to detect 113 photons from a 144-mode optical interferometer, achieving a sampling rate acceleration of 1024 --- representing an advancement of 37 photons and 10 orders of magnitude compared to the previous Jiuzhang system.\n",
    "\n",
    "The current state-of-the-art in experimentation and the growing need for QEC have spurred the development of inventive algorithms aimed at achieving the long-anticipated quantum advantage. The term \"near-term quantum computation\" has been introduced to encompass these quantum algorithms specifically tailored for quantum computing hardware expected to emerge in the coming years. Conversely, NISQ devices can only execute quantum circuits structured according to a specified graph topology, with each node representing qubits and gates typically operating on one or two qubits. Due to the inherent noise in gate operations, NISQ algorithms are inherently limited to shallow depths. It is important to clarify that NISQ is a hardware-centric concept, and does not inherently imply a temporal aspect. Nevertheless, it is important to identify that the term \"near-term\"\" is subjective and varies among researchers, as predictions regarding experimental progress are prone to human biases. Algorithms tailored for \"near-term\" hardware may become infeasible if hardware advancements fail to align with the experimental requirements of the algorithm. Readers interested in understanding better the current quantum technologies and their implementations can refer to [Understanding Quantum Technologies](https://www.oezratty.net/wordpress/2023/understanding-quantum-technologies-2023/).\n",
    "\n",
    "So, what are the means of harnessing the power given by NISQ computers? Since the capacity of a modern quantum computer is limited, many algorithms are devised in a hybrid quantum--classical manner. Such algorithms assign a classically demanding component to a quantum computer, while the tractable part is still carried out on a (sufficiently powerful) classical computer. The concept of these algorithms typically involves some variational updates to parameters in a parametrised quantum circuit (PQC), subsequently termed variational quantum algorithms (VQAs) (which is a subset of hybrid quantum--classical algorithms). Originally, the first two designs for VQAs were: the variational quantum eigensolver (VQE), initially proposed to solve quantum chemistry problems; and the quantujm approximate optimisation algorithm (QAOA), intended to solve combinatorial optimisation problems. Both of these algorithms can be considered as the parents of the entire VQA family.\n",
    "\n",
    "VQAs attempt to bridge the gap between quantum and classical, by leveraging the peak capabilities of current classical computers, and the potential of NISQ and near-term devices to obtain a significant computational advantage. Accounting for all the limitations imposed by NISQ computers necessitates employing either an optimisation-based or learning-based strategy, a task effectively tackled by VQAs. They serve as the quantum counterpart to highly successful machine learning techniques, such as neural networks. Furthermore, VQAs utilise classical optimisation techniques by employing PQCs, which are then optimised by classical optimisers. This method offers the benefit of maintaining a shallow quantum circuit depth, thereby reducing noise, unlike quantum algorithms designed for fault-tolerant systems. VQAs have been extensively explored across a wide array of applications, as illustrated in the figure below, virtually encompassing most of the envisioned uses for quantum computing. While they hold promise as a pathway to achieving quantum advantage in the near term, VQAs encounter significant challenges related to their trainability, accuracy, and efficiency.\n",
    "\n",
    "<img height=\"500\" src=\"figures/applications.png\" width=\"600\"/>\n",
    "\n",
    "## Overview of the Course\n",
    "\n",
    "This course will be a short introduction to Variational Quantum Algorithms (VQAs), through the use of Python and associated packages (namely [Qiskit](https://qiskit.org)).\n",
    "\n",
    "## Programming Environment\n",
    "\n",
    "We will be using Python for this course. The following packages will be necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.9 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 16:40:41) [MSC v.1916 64 bit (AMD64)]\n",
      "SciPy version: 1.14.1\n",
      "MatPlotLib version: 3.9.2\n",
      "NumPy version: 2.1.0\n",
      "Qiskit version: 1.2.0\n",
      "Qiskit_Algorithms version: 0.3.0\n",
      "Pylatexenc version: 2.10\n"
     ]
    }
   ],
   "source": [
    "import sys, scipy, matplotlib, numpy, qiskit, qiskit_algorithms, pylatexenc\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"SciPy version: {scipy.__version__}\")\n",
    "print(f\"MatPlotLib version: {matplotlib.__version__}\")\n",
    "print(f\"NumPy version: {numpy.__version__}\")\n",
    "print(f\"Qiskit version: {qiskit.__version__}\")\n",
    "print(f\"Qiskit_Algorithms version: {qiskit_algorithms.__version__}\")\n",
    "print(f\"Pylatexenc version: {pylatexenc.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Variational Quantum Algorithm\n",
    "\n",
    "A VQA is made up of separate modules that can be easily combined, improved and extended according to the developments in quantum technology. The different modules of a VQA are the:\n",
    "\n",
    "1. Objective Function\n",
    "2. Parametrised Quantum Circuit\n",
    "3. Measurement Technique\n",
    "4. Classical Optimiser\n",
    "\n",
    "In this work, the above modules are only briefly discussed in this course for the sake of brevity, further detail on constructing VQAs can be found in [Bharti et al.](https://link.aps.org/doi/10.1103/RevModPhys.94.015004). A diagram of the setup of a VQA can be seen in the figure below.\n",
    "\n",
    "<img height=\"500\" src=\"figures/vqa.png\" width=\"800\"/>\n",
    "\n",
    "The figure above is a diagrammatic representation of a VQA, with the four main modules: **a)** The objective function $O$, which encodes the problem to be solved; **b)** the PQC, in which the parameters $\\boldsymbol{\\theta}$ are variationally updated to minimise the objective function; **c)** the measurement technique, which involves basis changes and measurements needed to compute the objective function; and **d)** the classical optimiser that minimises the objective function while proposing a new set of variational parameters.\n",
    "\n",
    "## Objective Function\n",
    "\n",
    "When considering a quantum system, the Hamiltonian typically contains all of the relevant information of that system and generally forms its description. In quantum chemistry or many-body physics, the expectation value of the Hamiltonian can lead to the energy of the system, which is often used as the objective function to be optimised for a VQA. On the other hand, for combinatorial problems, and others which do not fall in the realm of real physical systems, the respective problem can be encoded into a Hamiltonian, allowing them to be solved on a quantum computer.\n",
    "\n",
    "The objective function $f$, defined in terms of an expectation value of a Hamiltonian, can be computed as\n",
    "\\begin{equation}\n",
    "    \\min_{\\boldsymbol{\\theta}} f\\left(\\boldsymbol{\\theta}, \\left\\{\\langle \\mathcal{H} \\rangle_{U(\\boldsymbol{\\theta})}\\right\\}\\right),\n",
    "\\end{equation}\n",
    "where $\\langle \\mathcal{H} \\rangle_{U(\\boldsymbol{\\theta})}$ denotes the expectation of the Hamiltonian $\\mathcal{H}$ in the quantum state generated by the unitary $U(\\boldsymbol{\\theta})$, which can be expanded as\n",
    "\\begin{equation}\n",
    "    \\langle \\mathcal{H} \\rangle_{U(\\boldsymbol{\\theta})} \\equiv \\bra{0}^{\\otimes n}U^\\dagger(\\boldsymbol{\\theta}) \\mathcal{H} U(\\boldsymbol{\\theta})\\ket{0}^{\\otimes n}.\n",
    "\\end{equation}\n",
    "The cost function is ubiquitously associated with the VQE algorithm. Once the Hamiltonian of the problem is determined --- along with the associated objective function --- the next step is to decompose it into a set of operators which are measurable on the quantum device. The most common of which is Pauli string decomposition, where such a decomposed Hamiltonian is given as\n",
    "\\begin{equation}\n",
    "    \\mathcal{H} = \\sum\\limits_{i=1}^{\\mathcal{P}} w_i P_i,\n",
    "\\end{equation}\n",
    "where $w_i$ is the weight of the Pauli string $P_i$. A Pauli string can be written as a tensor product of Pauli operators $\\sigma^x, \\sigma^y, \\sigma^z$ --- $P_i = \\bigotimes_{j = 1}^{n} \\sigma_i^{(j)}$, where $\\sigma_i^{(j)} \\in \\{\\mathbb{1}, \\sigma^x, \\sigma^y, \\sigma^z\\}$, with $\\mathbb{1}$ denoting the identity operator. As a result, the expectation value of the Hamiltonian can then be measured by evaluating the expectation value of each of the decomposed Pauli strings:\n",
    "\\begin{equation}\n",
    "    \\langle \\mathcal{H} \\rangle_{U(\\boldsymbol{\\theta})} = \\sum\\limits_{i=1}^{\\mathcal{P}} w_i \\langle P_i \\rangle_{U(\\boldsymbol{\\theta})}.\n",
    "\\end{equation}\n",
    "This form of decomposed qubit Hamiltonians include molecules, spin chains, or other encoded models.\n",
    "\n",
    "Other objective functions can be used as well, such as the infidelity and the conditional value at risk (CVaR). In fact, any cost function that can be written in an operational form can be evaluated using a quantum computer, and thus used as an objective function.\n",
    "\n",
    "## Parametrised Quantum Circuit\n",
    "\n",
    "Once the objective function is determined, the next component of the VQA is the quantum circuit that is capable of preparing the quantum state that matches with the minimum of the objective function. The quantum state is prepared via a unitary operation that depends on a set of parameters, called a parametrised quantum circuits (PQCs).\n",
    "\n",
    "The PQC is generally applied after preparing an initial state $\\ket{\\psi_0}$, such that the quantum state is then\n",
    "\\begin{equation}\n",
    "    \\ket{\\psi(\\boldsymbol{\\theta})} = U(\\boldsymbol{\\theta})\\ket{\\psi_0},\n",
    "\\end{equation}\n",
    "where $\\boldsymbol{\\theta}$ are the variational parameters. Typically, the initial state is simply the all-zero state $\\ket{0}^{\\otimes n} = \\ket{00\\cdots0}$, where $n$ is the number of qubits. However, as an example, the QAOA uses an initial state starting from a uniform superposition of all qubits in the computational basis, such that \n",
    "\\begin{equation}\n",
    "    \\ket{D} = H^{\\otimes n} \\ket{0}^{\\otimes n},\n",
    "\\end{equation}\n",
    "where $H = (\\sigma^x + \\sigma^z)/\\sqrt{2}$ is the Hadamard gate. In quantum chemistry models, the initial state is generally chosen to correspond to the Hartree--Fock approximation. The purpose of preparing a good initial state is so that the algorithm is able to optimise the parameters $\\boldsymbol{\\theta}$ close to an optimal point, helping the overall convergence of the algorithm.\n",
    "\n",
    "Following initial state preparation, the choice of the unitary ansatz $U(\\boldsymbol{\\theta})$ considerably affects the performance of the VQA. A necessary condition to find the minimum is that the quantum state that minimises the objective function lies within the subspace of the Hilbert space traversable by the PQC. However, more complex unitaries generally lead to deeper and non-local circuits, which are more susceptible to errors. As a result, unitary ansatz selection normally falls into two main categories, problem-inspired or hardware-efficient, depending on their structure.\n",
    "\n",
    "We will mainly discuss fixed-structure ans\\\"{a}tze --- for a detailed review on different types of ans\\\"{a}tze, refer to [Tilly et al.](https://www.sciencedirect.com/science/article/pii/S0370157322003118) and [Bharti et al.](https://link.aps.org/doi/10.1103/RevModPhys.94.015004). However, it is important to note that there are adaptive-structure ans\\\"{a}tze.\n",
    "\n",
    "### Problem-Inspired Ansätze\n",
    "\n",
    "Given a Hermitian operator $g$, one can create a unitary operator describing the evolution of $g$ as a function of time $t$,\n",
    "\\begin{equation}\n",
    "    U(t) = e^{-\\imath g t}.\n",
    "\\end{equation}\n",
    "While $g$ can be any Hermitian operator, such as a Pauli operator, it is often taken to be the Hamiltonian which describes the unitary evolution of the system. Therefore,\n",
    "\\begin{equation}\n",
    "    U(t) = e^{-\\imath \\mathcal{H} t},\n",
    "\\end{equation}\n",
    "which can be decomposed into Pauli operators. However, it is generally not straightforward to implement unitary evolution on a quantum device, and as a consequence, a Suzuki-Trotter decomposition is needed to implement an approximation of the unitary operator. If we once again look at Pauli decomposition, and the sets of operators $\\left\\{P_i\\right\\}$ are chosen such that they are non-commuting, then $e^{-\\imath P_i t}$ can be easily implemented. The full evolution over $t$ of such a decomposition can then be implemented as $k$ equal-sized steps\n",
    "\\begin{equation}\n",
    "    e^{-\\imath \\mathcal{H} t} = \\lim\\limits_{k \\rightarrow \\infty} \\left( \\prod\\limits_i e^{-\\imath \\frac{w_i P_i t}{k}} \\right)^k.\n",
    "\\end{equation}\n",
    "An approximation is then carried out by taking $k$ finite steps, such that\n",
    "\\begin{equation}\n",
    "    e^{-\\imath \\mathcal{H} t} \\approx \\left( \\prod\\limits_i e^{-\\imath \\frac{w_i P_i t}{k}} \\right)^k.\n",
    "\\end{equation}\n",
    "This is known as _trotterisation_, however in practice, this generally results in challenging experimental implementations of such ans\\\"{a}tze. Knowledge about the physics of the Hamiltonian can substantially reduce the complexity, depth and number of gates of the PQC using trotterised ans\\\"{a}tze.\n",
    "\n",
    "The variational Hamiltonian ansatz (VHA) is another ansatz which is motivated by adiabatic state preparation, where it was developed to improve convergence and limit the number of variational parameters. Given a Hamiltonian that is described in a Pauli decomposition, then the unitary can be described by\n",
    "\\begin{equation}\n",
    "    U(\\boldsymbol{\\theta}) = \\prod\\limits_i e^{-\\imath \\theta_i P_i},\n",
    "\\end{equation}\n",
    "which can be repeated multiple times in the PQC, corresponding to multiple _layers_ of the unitary operator in the PQC.\n",
    "\n",
    "The quantum approximate optimization algorithm (QAOA) is one of the original algorithms designed to perform well in the NISQ era, to solve combinatorial optimisation problems. While it can be thought of as a type of VQA, it can also be considered as a type of PQC --- that is typically referred to as the quantum alternating operator ansatz. Given a cost function $C$ that encodes a combinatorial problem in bit strings forming the computational basis vectors $\\ket{i}$, then the problem Hamiltonian $\\mathcal{H}_P$ is defined as\n",
    "\\begin{equation}\n",
    "    \\mathcal{H}_P \\equiv \\sum\\limits_{i=1}^n C(i)\\ketbra{i},\n",
    "\\end{equation}\n",
    "and the mixing Hamiltonian $\\mathcal{H}_M$ as\n",
    "\\begin{equation}\n",
    "    \\mathcal{H}_M \\equiv \\sum\\limits_{i=1}^n \\sigma_i^x,\n",
    "\\end{equation}\n",
    "with the initial state given as the uniform superposition state $\\ket{D}$. The final quantum state is given by applying alternating unitaries generated by $\\mathcal{H}_P$ and $\\mathcal{H}_M$ on the initial state $p$-times, such that\n",
    "\\begin{equation}\n",
    "    \\ket{\\psi(\\boldsymbol{\\gamma}, \\boldsymbol{\\beta})} \\equiv e^{-\\imath \\beta_p \\mathcal{H}_M}e^{-\\imath \\gamma_p \\mathcal{H}_P} \\cdots e^{-\\imath \\beta_1 \\mathcal{H}_M}e^{-\\imath \\gamma_1 \\mathcal{H}_P}\\ket{D}.\n",
    "\\end{equation}\n",
    "The cost function is then given by\n",
    "\\begin{equation}\n",
    "    C(\\boldsymbol{\\gamma}, \\boldsymbol{\\beta}) \\equiv \\bra{\\psi(\\boldsymbol{\\gamma}, \\boldsymbol{\\beta})} \\mathcal{H}_P \\ket{\\psi(\\boldsymbol{\\gamma}, \\boldsymbol{\\beta})},\n",
    "\\end{equation}\n",
    "where $\\boldsymbol{\\gamma}$ and $\\boldsymbol{\\beta}$ are the variational parameters, and $p$ represents the number of layers of the QAOA. The minimisation at level $p-1$ is a constrained version of the minimisation at level $p$, meaning that the algorithm improves monotonically with $p$.\n",
    "\n",
    "### Hardware-Efficient Ansätze\n",
    "\n",
    "The problem-inspired ans\\\"{a}tze are constructed from the underlying physics of the model to be solved. However, given that we are in the NISQ era, we are required to address experimental limitations that consist of qubit connectivity, restricted gate set, decoherence and gate errors, among others. Thus, the concept of hardware-efficient ans\\\"{a}tze was then introduced, with the idea that they respect limited qubit connectivity, with simple and relatively local gates, which are set up so as to form a sequence of one-qubit gates, followed by an entangling sequence of two-qubit gates. These layers are then repeated enough times to generate a sufficient amount of entanglement and enable the exploration of a large portion of the Hilbert space.\n",
    "\n",
    "A hardware-efficient unitary operator with $L$ layers can be written as\n",
    "\\begin{equation}\n",
    "    U(\\boldsymbol{\\theta}) = \\prod\\limits_{k=1}^L U_k(\\boldsymbol{\\theta}_k)W_k,\n",
    "\\end{equation}\n",
    "where $U_k(\\boldsymbol{\\theta}_k) = \\exp(-\\imath \\boldsymbol{\\theta}_k V_k)$ is a unitary generated by a non-parametrised Hermitian operator $V_k$, generally being single-qubit rotation gates, and $W_k$ is an entangling gate, which is ordinarily non-parametrised. The entangling gate generally depends on the native gate set of the quantum device, for example, for superconducting architectures the typical native two-qubit entangling gate is the CNOT or C$Z$ gate, or $XX$ gates for trapped ions. An example of a hardware-efficient ansatz is shown below.\n",
    "\n",
    "<img height=\"300\" src=\"figures/he_ansatz.png\" width=\"600\"/>\n",
    "\n",
    "### Other Ansätze\n",
    "\n",
    "In the context of quantum chemistry, the unitary coupled cluster (UCC) ansatz is in fact the ansatz of choice in the first proposed VQE. Coupled cluster theory is a post Hartree--Fock method that aims at recovering a portion of electron correlation energy by evolving an initial wave function. \n",
    "\n",
    "Moreover, rather than choosing between problem-inspired and hardware-efficient ans\\\"{a}tze, one can find a middle ground. There exists a midway approach of a VHA that utilises hardware-efficient gates, and are typically symmetry-preserving methods, which has two main benefits: it restricts the size of the Hilbert space, reducing the risk of barren plateaus (BPs) and potentially speeding up optimisation; and may avoid degeneracies and localised errors. The particular symmetry-preservation that will be considered is particle number conservation, with the most commonly employed entangling gate being\n",
    "\\begin{equation}\n",
    "    A(\\theta, \\phi) = \\left( \n",
    "    \\begin{array}{cccc}\n",
    "        1 & 0 & 0 & 0 \\\\\n",
    "        0 & \\cos(\\theta) & -e^{-\\imath \\phi}\\sin(\\theta) & 0 \\\\\n",
    "        0 & e^{\\imath \\phi}\\sin(\\theta) & \\cos{\\theta} & 0 \\\\\n",
    "        0 & 0 & 0 & 1 \\\\\n",
    "    \\end{array} \n",
    "    \\right),\n",
    "\\end{equation}\n",
    "which can be decomposed into three CNOT gates and two rotation gates. Note that symmetry constraints can also be achieved by modifying the cost function, typically in the form of including Lagrangian multipliers.\n",
    "\n",
    "While only fixed-structure ans\\\"{a}tze have been mentioned, it is important to acknowledge the concept of adaptive-structure ans\\\"{a}tze. The main idea is to construct a circuit that adapts to the problem at hand. The most common procedure is to iteratively add new operators to the circuit based on their contribution to the overall energy, generally by minimising an alternative cost function after completing each iteration of a regular VQA.\n",
    "\n",
    "## Measurement Technique\n",
    "\n",
    "A quantum computer can only supply information via measuring quantum states. Typically, the expectation value of a Hamiltonian is required to minimise an objective function. This requires a unitary transformation on the quantum state to the diagonal basis of the Hamiltonian. However, in principle, this is hard to achieve on a NISQ device. The purpose of transforming a Hamiltonian into Pauli strings is because their expectation value can be easily determined on a quantum computer. The computational basis states $\\ket{0}$ and $\\ket{1}$ can be measured to give the expectation value of the $\\sigma^z$ operator,\n",
    "\\begin{equation}\n",
    "    \\langle \\sigma^z \\rangle = p_0 - p_1 = 2p_0 - 1,\n",
    "\\end{equation}\n",
    "where $p_0$, $p_1$ are the probabilities to measure the qubit in state $\\ket{0}$, $\\ket{1}$, respectively. The last equality is due to $p_0 + p_1 = 1$. Subsequently, measurements of $\\sigma^x$ and $\\sigma^y$ can be carried out by transforming the operators\n",
    "\\begin{equation}\n",
    "    \\sigma^x = H \\sigma^z H,\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    \\sigma^y = N_s H \\sigma^z H N_s^\\dagger,\n",
    "\\end{equation}\n",
    "where $N_s = \\sqrt{\\sigma^z}$ and $H$ is the Hadamard gate. The expectation values can be determined by applying the transforming gates on the states before measuring in the computational basis, so that\n",
    "\\begin{equation}\n",
    "    \\langle \\sigma^x \\rangle = \\bra{\\psi}\\sigma^x\\ket{\\psi} = \\bra{\\psi}H \\sigma^z H\\ket{\\psi} = 2p_0 - 1,\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    \\langle \\sigma^y \\rangle = \\bra{\\psi}\\sigma^y\\ket{\\psi} = \\bra{\\psi}N_s H \\sigma^z H N_s^\\dagger\\ket{\\psi} = 2p_0 - 1.\n",
    "\\end{equation}\n",
    "Thus, any Pauli string can be evaluated by, if necessary, transforming each individual qubit $k$ into the $\\sigma^z$ basis and then measuring:\n",
    "\\begin{align}\n",
    "    \\langle P \\rangle_{U} &= \\left\\langle \\prod\\limits_k \\sigma^{f(k)}(k) \\right\\rangle_U = \\left\\langle \\prod\\limits_k \\sigma^z(k) \\right\\rangle_{VU} = \\prod\\limits_k (2p_0(k) - 1),\n",
    "\\end{align}\n",
    "where $V$ is the product of one-qubit rotations given by $x$- and $y$-rotations, depending on the Pauli terms $\\sigma^{f(k)}$ at qubit $k$. \n",
    "\n",
    "In many cases, the number of Pauli strings can be so large that it becomes costly to evaluate each one of them individually. Recently, various approaches for efficiently measuring the expectation value of a Hamiltonian have been proposed, such as grouping different Pauli strings together to enable simultaneous measurement.\n",
    "\n",
    "Other techniques exist, such as measuring the overlap of a quantum state with a unitary operator, using approaches such as the Hadamard test, or direct implementation techniques. Another method is shadow tomography, which involves measuring the classical shadow of a quantum state to extract the expectation value of observables.\n",
    "\n",
    "## Classical Optimiser\n",
    "\n",
    "So far, only the components of a VQA that operate on a quantum device have been discussed. The final piece is the classical optimisation loop, which updates the variational parameters of the PQC, tying everything together to create a hybrid quantum--classical loop. The specific choice of an optimiser greatly impacts the performance of the VQA. In principle, one can apply any method available in the entire field of multivariate optimisation. However, the NISQ era continues to be a limitation due to short coherence times that prevent the efficient implementation of analytical gradient circuits, and result in lengthy computation times for numerous function evaluations needed to calculate the objective function. At the same time, the results from a quantum computer are noisy, both from statistical and quantum device errors, necessitating the use of an optimiser that is noise-resistant. As a result, most of the frequently employed classical optimisers are not adequate for VQAs, and as a result, new optimisation algorithms and techniques are being developed to optimise PQCs and their corresponding objective functions.\n",
    "\n",
    "Classical optimisers can generally be categorised into two classes: gradient-based and gradient free. We shall go through a brief description of the common approaches to both types.\n",
    "\n",
    "### Gradient-Based Approach\n",
    "\n",
    "The most common approach to minimise an objective function $f(\\boldsymbol{\\theta})$ is through evaluating its gradient. The gradient of a function indicates the direction in which it exhibits the largest rate of change, also called the direction of steepest _ascent_. Suppose we start with an initial set of $K$ parameters $\\boldsymbol{\\theta}^{(0)}$, and iteratively update $\\boldsymbol{\\theta}^{(n)}$ over $n$ iterations. One such update rule is\n",
    "\\begin{equation}\n",
    "    \\boldsymbol{\\theta}^{(n+1)} = \\boldsymbol{\\theta}^{(n)} - \\eta \\nabla f(\\boldsymbol{\\theta}^{(n)}),\n",
    "\\end{equation}\n",
    "where $\\eta$ is a hyperparameter, denoted as the learning rate (which can be a positive series $\\eta_n$ converging to zero), and $\\nabla = (\\partial_1,\\dots,\\partial_K)$ representing the gradient of the objective function, with $\\partial_i \\equiv \\frac{\\partial}{\\partial \\theta_i}$. This translates to optimising each parameter $\\theta_i$ via\n",
    "\\begin{equation}\n",
    "    \\theta_i^{(n+1)} = \\theta_i^{(n)} - \\eta \\partial_i f(\\boldsymbol{\\theta}^{(n)}),\n",
    "\\end{equation}\n",
    "using Einstein notation, whereby each parameter is nudged in the direction of steepest _descent_ due to the negative sign. The gradient can be estimated on a quantum computer by using different methods, the most common of which is using finite difference techniques:\n",
    "\\begin{equation}\n",
    "    \\partial_i f(\\boldsymbol{\\theta}^{(n)}) \\approx \\frac{f(\\boldsymbol{\\theta}^{(n)} + \\epsilon \\boldsymbol{e}_i) - f(\\boldsymbol{\\theta}^{(n)} - \\epsilon \\boldsymbol{e}_i)}{2\\epsilon},\n",
    "\\end{equation}\n",
    "where $\\epsilon$ is a small number and $\\boldsymbol{e}_i$ is the unit vector in the $i^\\text{th}$ direction. The smaller the $\\epsilon$, the closer the approximation is to the true value of the gradient. On the other hand, a smaller $\\epsilon$ corresponds to a smaller difference $f(\\boldsymbol{\\theta}^{(n)} + \\epsilon \\boldsymbol{e}_i) - f(\\boldsymbol{\\theta}^{(n)} - \\epsilon \\boldsymbol{e}_i)$, which would in turn entail a larger number of samples from the quantum computer to achieve a good estimation of the gradient. Furthermore, $\\epsilon$ can be a function of $n$, i.e. $\\epsilon = \\epsilon^{(n)}$, that typically decreases with $n$, to obtain a finer resolution. Note that with this method, each iteration of the optimiser requires $2K$ evaluations of the cost function $f$.\n",
    "\n",
    "A noteworthy gradient-descent method that uses a stochastic approach is simultaneous perturbation stochastic approximation (SPSA). The basic premise of SPSA is to perturb all directions of the gradient evaluation simultaneously in each iteration. Specifically, let $\\Delta^{(n)}$ be a random perturbation vector, typically following the Bernoulli distribution $\\pm1$ with probability $0.5$. The stochastic perturbation gradient estimator is then given by\n",
    "\\begin{equation}\n",
    "    \\partial_i f(\\boldsymbol{\\theta}^{(n)}) \\approx \\frac{f(\\boldsymbol{\\theta}^{(n)} + \\epsilon \\Delta^{(n)}) - f(\\boldsymbol{\\theta}^{(n)} - \\epsilon \\Delta^{(n)})}{2\\epsilon \\Delta_i^{(n)}}.\n",
    "\\end{equation}\n",
    "Compared to finite difference techniques, SPSA requires only two evaluations to compute the update step at each iteration, making it much more suitable for NISQ algorithms.\n",
    "\n",
    "A schematic representation of gradient descent and stochastic gradient descent can be seen below.\n",
    "\n",
    "<img height=\"300\" src=\"figures/gradient_descent.png\" width=\"600\"/>\n",
    "\n",
    "The gradients of PQCs that are constructed via single-parameter unitary gates can be analytically determined using what are known as parameter shift rules. Consider a sequence of $N$ parametrised unitary operators, as derived from the alternating-layered ansatz,\n",
    "\\begin{equation}\n",
    "    U(\\boldsymbol{\\theta}) = \\prod_{i=0}^{N} U_i(\\theta_i),\n",
    "\\end{equation}\n",
    "and\n",
    "where without loss of generality we assume that the fixed gates are represented by $U_k(\\theta_k)$ such that $\\theta_k$ is fixed. Since each of these gates are unitary, then they can be represented by a generator $g_i$ such that\n",
    "\\begin{equation}\n",
    "    U_i(\\theta_i) = e^{-\\imath \\theta_i g_i},\n",
    "\\end{equation}\n",
    "and\n",
    "\\begin{equation}\n",
    "    \\partial_i U_i(\\theta_i) = -\\imath g_i e^{-\\imath \\theta_i g_i} = -\\imath e^{-\\imath \\theta_i g_i} g_i.\n",
    "\\end{equation}\n",
    "With a slight abuse of notation $\\ket{0} \\equiv \\ket{0}^{\\otimes n}$, a common cost function in most VQAs is\n",
    "\\begin{equation}\n",
    "    f(\\boldsymbol{\\theta}) = \\bra{0} U(\\boldsymbol{\\theta})^\\dagger \\mathcal{H} U(\\boldsymbol{\\theta}) \\ket{0},\n",
    "\\end{equation}\n",
    "in which the derivative with respect to $\\theta_i$ can be rewritten as\n",
    "\\begin{align}\n",
    "    \\partial_i f(\\boldsymbol{\\theta}) &= \\partial_i \\bra{0} U(\\boldsymbol{\\theta})^\\dagger \\mathcal{H} U(\\boldsymbol{\\theta}) \\ket{0} \\nonumber \\\\\n",
    "    &= \\bra{0} \\partial_i(U(\\boldsymbol{\\theta})^\\dagger \\mathcal{H} U(\\boldsymbol{\\theta})) \\ket{0} \\nonumber \\\\\n",
    "    &= \\bra{0} \\partial_i(U(\\boldsymbol{\\theta})^\\dagger) \\mathcal{H} U(\\boldsymbol{\\theta}) + U(\\boldsymbol{\\theta})^\\dagger \\mathcal{H} \\partial_i(U(\\boldsymbol{\\theta})) \\ket{0} \\nonumber \\\\\n",
    "    &= \\bra{\\psi_{i-1}} \\partial_i(U_i(\\theta_i)^\\dagger) \\mathcal{H}_{i+1} U_i(\\theta_i) + U_i(\\theta_i)^\\dagger \\mathcal{H}_{i+1} \\partial_i(U_i(\\theta_i)) \\ket{\\psi_{i-1}} \\nonumber \\\\\n",
    "    &= \\imath \\bra{\\psi_{i-1}} U_i(\\theta_i)^\\dagger g_i^\\dagger \\mathcal{H}_{i+1} U_i(\\theta_i) - U_i(\\theta_i)^\\dagger \\mathcal{H}_{i+1}g_i U_i(\\theta_i) \\ket{\\psi_{i-1}} \\nonumber \\\\\n",
    "    &= \\imath \\bra{\\psi_{i-1}} U_i(\\theta_i)^\\dagger (g_i\\mathcal{H}_{i+1} - \\mathcal{H}_{i+1} g_i) U_i(\\theta_i) \\ket{\\psi_{i-1}} \\nonumber \\\\\n",
    "    &= \\imath \\bra{\\psi_{i-1}} U_i(\\theta_i)^\\dagger [g_i, \\mathcal{H}_{i+1}] U_i(\\theta_i) \\ket{\\psi_{i-1}},\n",
    "\\end{align}\n",
    "where we have used the fact that $g_i = g_i^\\dagger$, and define $\\ket{\\psi_{i-1}} = \\left(\\prod_{j=0}^{i-1}U_j(\\theta_j)\\right) \\ket{0}$ and $\\mathcal{H}_{i+1} = \\left(\\prod_{j=i+1}^N U_j\\right)^\\dagger \\mathcal{H} \\left(\\prod_{j=i+1}^N U_j\\right)$. It was shown that if the generator $g_i$ has only two distinct eigenvalues $\\{\\pm r\\}$\\footnote{This inherently includes all single-qubit gates.} (that can be repeated), then\n",
    "\\begin{equation}\n",
    "    [g_i, \\mathcal{H}] = -\\imath r \\left( U_i^\\dagger\\left(\\frac{\\pi}{4r}\\right) \\mathcal{H} U_i\\left(\\frac{\\pi}{4r}\\right) -U_i^\\dagger\\left(-\\frac{\\pi}{4r}\\right) \\mathcal{H} U_i\\left(-\\frac{\\pi}{4r}\\right) \\right).\n",
    "\\end{equation}\n",
    "Putting the previous commutator in the parameter shift equation, we get\n",
    "\\begin{align}\n",
    "    \\partial_i f(\\boldsymbol{\\theta}) &= r\\bra{\\psi_{i-1}} U_i\\left(\\theta_i + \\frac{\\pi}{4r}\\right)^\\dagger \\mathcal{H}_{i+1} U_i\\left(\\theta_i + \\frac{\\pi}{4r}\\right)\\ket{\\psi_{i-1}} \\nonumber \\\\\n",
    "    & \\hspace{0.03cm} - r\\bra{\\psi_{i-1}}U_i\\left(\\theta_i - \\frac{\\pi}{4r}\\right)^\\dagger \\mathcal{H}_{i+1} U_i\\left(\\theta_i - \\frac{\\pi}{4r}\\right) \\ket{\\psi_{i-1}} \\nonumber \\\\\n",
    "    &= r\\left( f\\left(\\boldsymbol{\\theta} + \\frac{\\pi}{4r}e_i\\right) - f\\left(\\boldsymbol{\\theta} - \\frac{\\pi}{4r}e_i\\right) \\right).\n",
    "\\end{align}\n",
    "That is, by shifting the parameter $\\theta_i$ by $\\pm \\pi / 4r$ and measuring, one can exactly determine the gradient of $f$ with respect to $\\theta_i$. While the parameter shift rule looks quite similar to the finite difference rule, it uses a finite, not infinitesimal, shift and is in fact exact, e.g., in the special case where the generator is a Pauli rotation, $r = 1 / 2$.\n",
    "\n",
    "There are many more techniques which can be used to calculate the gradient of an objective function on a quantum device; such as the Broyden--Fletcher--Goldfarb--Shanno (BFGS) algorithm (typically used to obtain statevector results), that utilises Hessian information for gradient descent; sequential least squares programming (SLSQP), a quasi-Newton method that optimises successive second-order approximations of the objective function (via BFGS updates), with first-order approximations of the constraints; quantum natural gradient, that uses the Fisher information matrix; stochastic gradient descent, that replaces the exact partial derivative with an estimator of the partial derivative, similar to SPSA; quantum imaginary time evolution, invoking McLachlan’s variational principle; and quantum analytic descent, which reconstructs local regions of the energy landscape and carries out classical optimisation in that region.\n",
    "\n",
    "### Gradient-Free Approach\n",
    "\n",
    "One may prefer to employ an optimiser that does not rely on gradient information measured on a quantum computer, for a variety of technical reasons. There are methods based on evolutionary algorithms, which have been recently applied to VQAs, that show comparable performance to gradient-based approaches. Reinforcement learning is another method, originally used to optimise the parameters of QAOA.\n",
    "\n",
    "One such gradient-free approach is generalised simulated annealing (GSA), a probabilistic algorithm that searches for a global minimum, rather than a local one. GSA simulates an annealing process, similar to annealing molten metal in metallurgy, to ﬁnd a global minimum of the objective function. A visiting distribution is employed to generate a trial jump distance $\\Delta x^{(n)}$, which is proportional to a distorted Cauchy-Lorentz distribution\n",
    "\\begin{equation}\n",
    "    P_{q_v}\\left(\\Delta x^{(n)}\\right) \\propto \\frac{\\left(T_{q_v}^{(n)}\\right)^{-\\frac{D}{3-q_v}}}{\\left( 1 + (q_v - 1)\\frac{\\left(\\Delta x^{(n)}\\right)^2}{\\left(T_{q_v}^{(n)}\\right)^\\frac{2}{3-q_v}} \\right)^{\\frac{1}{q_v-1} + \\frac{D-1}{2}}},\n",
    "\\end{equation}\n",
    "where $n$ is the iteration, and $T_{q_v}^{(n)}$ is the artificial temperature, with both the temperature and visiting distribution controlled by the parameter $q_v$. The trial jump is automatically accepted if it is downhill, and possibly accepted according to an acceptance probability if it is uphill. A generalised Metropolis algorithm is used for the acceptance probability:\n",
    "\\begin{equation}\n",
    "    p_{q_a} = \\min\\left\\{1, \\left(1 - (1 - q_a) \\beta \\Delta E\\right)^\\frac{1}{1-q_a}\\right\\},\n",
    "\\end{equation}\n",
    "where $\\beta \\equiv 1 / T$, $\\Delta E$ is the proposed energy difference, with the acceptance probability controlled by the parameter $q_a$. The artiﬁcial temperature is decreased according to\n",
    "\\begin{equation}\n",
    "    T_{q_v}^{(n)} = T_{q_v}^{(0)}\\frac{2^{q_v - 1} - 1}{(2 + n)^{q_v - 1} - 1}.\n",
    "\\end{equation}\n",
    "The GSA algorithm can be supplemented with a local optimiser, which activates either after each newly accepted position, or once the GSA algorithm terminates upon meeting a criterion.\n",
    "\n",
    "Lastly, sequential minimal optimisation is another technique. Consider a PQC where; the parameters are independent of one another; is composed solely of fixed unitary gates and parametrised rotation gates; and the cost function is defined as a weighted sum of $K$ expectation values\n",
    "\\begin{equation}\n",
    "    f(\\boldsymbol{\\theta}) = \\sum_{i=1}^K w_k \\bra{\\psi_k} U^\\dagger(\\boldsymbol{\\theta}) \\mathcal{H}_k U(\\boldsymbol{\\theta}) \\ket{\\psi_k},\n",
    "\\end{equation}\n",
    "then, if $\\boldsymbol{\\theta}^{(n)}$ represents the parameters of the $n^\\text{th}$ iteration, where $U_j^{(n)}(\\theta_j)$ is the unitary $U(\\boldsymbol{\\theta})$ with all parameters fixed except $\\theta_j$, and similarly for the cost function $f_j^{(n)}(\\theta_j)$, then we have\n",
    "\\begin{align}\n",
    "    f_j^{(n)}(\\theta_j) &= \\sum_{i=1}^K w_k \\bra{\\psi_k} U_j^{(n)\\dagger}(\\boldsymbol{\\theta}) \\mathcal{H}_k U_j^{(n)}(\\boldsymbol{\\theta}) \\ket{\\psi_k} \\nonumber \\\\\n",
    "    &= \\alpha_j^{(n)} \\sin(\\theta_j + \\beta_j^{(n)}) + \\gamma_j^{(n)},\n",
    "\\end{align}\n",
    "where $\\alpha_j^{(n)}$, $\\beta_j^{(n)}$ and $\\gamma_j^{(n)}$ denote constants independent of $\\theta_j$. The previous equation signifies that $f_j^{(n)}(\\theta_j)$ is simply a sine curve with period $2\\pi$, where the constants can be determined by evaluating $f_j^{(n)}(\\theta_j)$ at three different points $\\theta_j$ (typically chosen to be $\\theta_j^{(n)}$ and $\\theta_j^{(n)} \\pm \\pi/2$). Then $\\theta_j$ can be found by minimising $f_j^{(n)}(\\theta_j)$, such that $\\theta_j= \\arg\\min_{\\theta_j} f_j^{(n)}(\\theta_j)$. The optimisation is carried out sequentially, or randomly, over all indices $j$, until convergence. Note that $f(\\boldsymbol{\\theta})$ needs to be reevaluated every so often, since the minimum is calculated from the sine curve, and errors will accumulate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
